{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4RsNZfCn5DIALlqv5vk/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbelAdissu/MNIST_Model_Selection_And_Evaluation/blob/main/MNIST_Model_Selection_And_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Importing Libraries and Loading the Data"
      ],
      "metadata": {
        "id": "EEvfmnV6BWDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml(\"mnist_784\", version=1)\n",
        "X = mnist.data\n",
        "y = mnist.target\n"
      ],
      "metadata": {
        "id": "hOboK3Mc5JWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "In this cell, we import necessary libraries for data manipulation, model selection, and evaluation.\n",
        "We load the MNIST dataset using fetch_openml from scikit-learn and store the features in X and the labels in y."
      ],
      "metadata": {
        "id": "-IcNLbODBcf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Defining Models and Hyperparameter Grids"
      ],
      "metadata": {
        "id": "XXNdm14tBhkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Define hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance']\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "bPoFMBnuBaEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "In this cell, we define two machine learning models, Logistic Regression and K-Nearest Neighbors, and store them in the models dictionary.\n",
        "We also specify hyperparameter grids for each model. These grids contain values for hyperparameters like C for Logistic Regression and n_neighbors and weights for K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "efvvmGjbCTse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Defining Accuracy Metrics"
      ],
      "metadata": {
        "id": "iQg3xOV_CX1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define accuracy metrics\n",
        "scoring = {\n",
        "    'Accuracy': 'accuracy',\n",
        "    'F1 Score': 'f1_macro',\n",
        "    'Precision': 'precision_macro',\n",
        "    'Recall': 'recall_macro'\n",
        "}\n"
      ],
      "metadata": {
        "id": "T1hhllxoBrkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "In this cell, we define a dictionary named scoring that maps different accuracy metrics to their respective scoring functions. The metrics include Accuracy, F1 Score, Precision, and Recall."
      ],
      "metadata": {
        "id": "59BQnEW5CcOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Splitting Data into Training and Testing Sets"
      ],
      "metadata": {
        "id": "NiJ2ptzgCnZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "YCun_LmPCbMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Here, we split the dataset into training and testing sets using train_test_split. The training set (X_train and y_train) contains 80% of the data, and the testing set (X_test and y_test) contains 20%. The random_state parameter ensures reproducibility.\n",
        "In the next cells, we will continue with the code to perform model selection and hyperparameter tuning using k-fold cross-validation, as well as evaluating the models using different accuracy metrics."
      ],
      "metadata": {
        "id": "KS-imz7ECyfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Model Selection with Cross-Validation"
      ],
      "metadata": {
        "id": "ll-hoeq2K1TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of folds for cross-validation\n",
        "num_folds = 5\n",
        "\n",
        "# Create a KFold cross-validation splitter\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Loop over each model\n",
        "for model_name, model in models.items():\n",
        "    param_grid = param_grids[model_name]\n",
        "\n",
        "    # Create a GridSearchCV object with k-fold cross-validation\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit='Accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_models[model_name] = best_model\n",
        "\n",
        "    print(f\"Best {model_name} Model:\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "sjENERs_EPvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "In this cell, we set the number of folds for k-fold cross-validation to 5 using num_folds.\n",
        "We create a KFold object, kf, which will be used to split the training data into folds for cross-validation. The shuffle parameter ensures that the data is shuffled before splitting, and random_state ensures reproducibility.\n",
        "We loop over each model defined earlier and create a GridSearchCV object for hyperparameter tuning with k-fold cross-validation. This allows us to find the best hyperparameters for each model while considering multiple accuracy metrics.\n",
        "The best model and its hyperparameters are stored in the best_models dictionary, and the best parameters and accuracy are printed for each model."
      ],
      "metadata": {
        "id": "mNWQ1N6kK6cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Evaluating Models on Test Data"
      ],
      "metadata": {
        "id": "rsn-q5bCK-kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best models on the test data\n",
        "for model_name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Evaluation for {model_name}:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"F1 Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "juEoPZfSK4e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "In this cell, we evaluate the best models selected during hyperparameter tuning on the test data.\n",
        "We use each model to make predictions on the test set and calculate various accuracy metrics, including Accuracy, F1 Score, Precision, and Recall.\n",
        "The results for each model are printed, providing a comprehensive evaluation of their performance on the test data."
      ],
      "metadata": {
        "id": "e_aMR7YyLIzR"
      }
    }
  ]
}